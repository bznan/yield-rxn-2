{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import json\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit\n",
    "from rxntorch.containers.reaction import Rxn\n",
    "from rxntorch.containers.molecule import Mol\n",
    "from rxntorch.containers.dataset import RxnGraphDataset as RxnGD\n",
    "from rxntorch.utils import collate_fn\n",
    "from rxntorch.models.yield_network import YieldNet as RxnNet, YieldTrainer as RxnTrainer\n",
    "from rxntorch.models import yield_network\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='output/pre-trained-models/model_architecture/gat_supervised_contextpred.pth'\n",
    "#model= net\n",
    "#optimizer = opt.Adam(model.parameters(), lr=args.lr, betas=betas, weight_decay=0)\n",
    "device = torch.device('cpu')\n",
    "#checkpoint = torch.load(PATH, map_location=device)\n",
    "#model.load_state_dict(checkpoint.state_dict())\n",
    "model = torch.load(PATH, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_embedding1.weight\n",
      "x_embedding2.weight\n",
      "gnns.0.att\n",
      "gnns.0.bias\n",
      "gnns.0.weight_linear.weight\n",
      "gnns.0.weight_linear.bias\n",
      "gnns.0.edge_embedding1.weight\n",
      "gnns.0.edge_embedding2.weight\n",
      "gnns.1.att\n",
      "gnns.1.bias\n",
      "gnns.1.weight_linear.weight\n",
      "gnns.1.weight_linear.bias\n",
      "gnns.1.edge_embedding1.weight\n",
      "gnns.1.edge_embedding2.weight\n",
      "gnns.2.att\n",
      "gnns.2.bias\n",
      "gnns.2.weight_linear.weight\n",
      "gnns.2.weight_linear.bias\n",
      "gnns.2.edge_embedding1.weight\n",
      "gnns.2.edge_embedding2.weight\n",
      "gnns.3.att\n",
      "gnns.3.bias\n",
      "gnns.3.weight_linear.weight\n",
      "gnns.3.weight_linear.bias\n",
      "gnns.3.edge_embedding1.weight\n",
      "gnns.3.edge_embedding2.weight\n",
      "gnns.4.att\n",
      "gnns.4.bias\n",
      "gnns.4.weight_linear.weight\n",
      "gnns.4.weight_linear.bias\n",
      "gnns.4.edge_embedding1.weight\n",
      "gnns.4.edge_embedding2.weight\n",
      "batch_norms.0.weight\n",
      "batch_norms.0.bias\n",
      "batch_norms.0.running_mean\n",
      "batch_norms.0.running_var\n",
      "batch_norms.0.num_batches_tracked\n",
      "batch_norms.1.weight\n",
      "batch_norms.1.bias\n",
      "batch_norms.1.running_mean\n",
      "batch_norms.1.running_var\n",
      "batch_norms.1.num_batches_tracked\n",
      "batch_norms.2.weight\n",
      "batch_norms.2.bias\n",
      "batch_norms.2.running_mean\n",
      "batch_norms.2.running_var\n",
      "batch_norms.2.num_batches_tracked\n",
      "batch_norms.3.weight\n",
      "batch_norms.3.bias\n",
      "batch_norms.3.running_mean\n",
      "batch_norms.3.running_var\n",
      "batch_norms.3.num_batches_tracked\n",
      "batch_norms.4.weight\n",
      "batch_norms.4.bias\n",
      "batch_norms.4.running_mean\n",
      "batch_norms.4.running_var\n",
      "batch_norms.4.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "for i in model.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-ud', '--use_domain'], dest='use_domain', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='use domain features or not', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"-p\", \"--dataset_path\", type=str, default='./data/', help=\"train dataset\")\n",
    "parser.add_argument(\"-mp\",\"--mol_path\", type=str, default='doyle_reaction_mols', help=\"path to mol files\")\n",
    "\n",
    "parser.add_argument(\"-c\", \"--train_dataset\", required=True, type=str, help=\"train dataset\")\n",
    "parser.add_argument(\"-t\", \"--test_dataset\", type=str, default=None, help=\"test set\")\n",
    "parser.add_argument(\"-op\", \"--output_path\", type=str, default='./output/', help=\"saved model path\")\n",
    "parser.add_argument(\"-o\", \"--output_name\", required=True, type=str, help=\"e.g. rxntorch.model\")\n",
    "parser.add_argument(\"-ds\", \"--train_split\", type=float, default=0.7, help=\"Ratio of samples to reserve for test data\")\n",
    "parser.add_argument(\"-vs\", \"--valid_split\", type=float, default=0.333, help=\"Ratio of samples to reserve for valid data\")\n",
    "parser.add_argument(\"-dr\", \"--dropout_rate\", type=float, default=0.333, help=\"Ratio of samples to reserve for valid data\")\n",
    "\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1, help=\"number of batch_size\")\n",
    "parser.add_argument(\"-tb\", \"--test_batch_size\", type=int, default=None, help=\"batch size for evaluation\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, help=\"number of epochs\")\n",
    "parser.add_argument(\"-hs\", \"--hidden\", type=int, default=200, help=\"hidden size of model layers\")\n",
    "parser.add_argument(\"-l\", \"--layers\", type=int, default=3, help=\"number of layers\")\n",
    "\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-2, help=\"learning rate of the optimizer\")\n",
    "parser.add_argument(\"-lrd\", \"--lr_decay\", type=float, default=0.9,\n",
    "                    help=\"Decay factor for reducing the learning rate\")\n",
    "parser.add_argument(\"-lrs\", \"--lr_steps\", type=int, default=10000,\n",
    "                    help=\"Number of steps between learning rate decay\")\n",
    "parser.add_argument(\"-awd\",\"--adam_weight_decay\", type=float, default=0.0, help=\"weight_decay of adam\")\n",
    "parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"adam first beta value\")\n",
    "parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"adam second beta value\")\n",
    "parser.add_argument(\"-gc\", \"--grad_clip\", type=float, default=None, help=\"value for gradient clipping\")\n",
    "parser.add_argument(\"-pw\", \"--pos_weight\", type=float, default=None, help=\"Weights positive samples for imbalance\")\n",
    "\n",
    "parser.add_argument(\"-w\", \"--num_workers\", type=int, default=4, help=\"dataloader worker size\")\n",
    "parser.add_argument(\"--with_cuda\", type=bool, default=True, help=\"training with CUDA: true, or false\")\n",
    "parser.add_argument(\"--cuda_devices\", type=int, nargs='*', default=None, help=\"CUDA device ids\")\n",
    "\n",
    "parser.add_argument(\"--log_freq\", type=int, default=50, help=\"printing loss every n iter: setting n\")\n",
    "parser.add_argument(\"--seed\", type=int, default=12, help=\"random seed\")\n",
    "parser.add_argument(\"-ud\",\"--use_domain\", type=str, required='True', help=\"use domain features or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/21 11:37:08: ------------------------------------Dataset-------------------------------------\n",
      "02/02/21 11:37:08: Loading Dataset suzuki_reactions_data.json in data/\n",
      "02/02/21 11:37:19: Dataset contains 0 total samples\n",
      "02/02/21 11:37:28: Dataset contains 4620 total samples\n",
      "02/02/21 11:37:30: Dataset contains 4620 total samples\n",
      "02/02/21 11:37:30: Loading Dataset doyle_reactions_data.json in data/\n",
      "02/02/21 11:37:40: Dataset contains 4599 total samples\n",
      "02/02/21 11:37:49: Dataset contains 4599 total samples\n",
      "02/02/21 11:37:51: Dataset contains 4599 total samples\n",
      "02/02/21 11:37:51: Loading Dataset az_reactions_data.json in data/\n",
      "02/02/21 11:38:00: Dataset contains 0 total samples\n",
      "02/02/21 11:38:09: Dataset contains 0 total samples\n",
      "02/02/21 11:38:10: Dataset contains 757 total samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 29]),\n",
       " torch.Size([66, 6]),\n",
       " torch.Size([64, 15]),\n",
       " torch.Size([64, 15]),\n",
       " torch.Size([1, 118]),\n",
       " torch.Size([15, 15, 15]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#args = parser.parse_args()\n",
    "data_type='doyle'\n",
    "args = parser.parse_args(args=['-p','data/','-c', data_type+'_reactions_data.json', '-o', 'model.01','-ud', 'False', '-mp', data_type+'_reaction_mols'])\n",
    "random_seed=args.seed\n",
    "args.output_name=\"reactivity.model\"\n",
    "\n",
    "if not os.path.exists(args.output_path):\n",
    "    os.mkdir(args.output_path)\n",
    "outputfile = os.path.join(args.output_path, args.output_name)\n",
    "logfile = '.'.join((args.output_name, \"log\"))\n",
    "logpath = os.path.join(args.output_path, logfile)\n",
    "logging.basicConfig(level=logging.INFO, style='{', format=\"{asctime:s}: {message:s}\",\n",
    "                    datefmt=\"%m/%d/%y %H:%M:%S\", handlers=(\n",
    "                    logging.FileHandler(logpath), logging.StreamHandler()))\n",
    "\n",
    "logging.info(\"{:-^80}\".format(\"Dataset\"))\n",
    "su_dataset = RxnGD('suzuki_reactions_data.json', mol_path=args.mol_path, path=args.dataset_path)\n",
    "dy_dataset = RxnGD('doyle_reactions_data.json', mol_path=args.mol_path, path=args.dataset_path)\n",
    "az_dataset = RxnGD('az_reactions_data.json', mol_path=args.mol_path, path=args.dataset_path)\n",
    "\n",
    "sample = dy_dataset[3]\n",
    "sample['atom_feats'].shape, sample['bond_feats'].shape, sample['atom_graph'].shape,sample['bond_graph'].shape,sample['domain_feats'].shape,sample[\"binary_feats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3696.0 924\n",
      "3679 920\n",
      "378.5 378.5\n"
     ]
    }
   ],
   "source": [
    "p=0.8;p0=0.5\n",
    "n1=4620;n2=4599;n3=757\n",
    "print(p*n1, round((1-p)*n1))\n",
    "print(round(p*n2),round( (1-p)*n2))\n",
    "print(p0*n3, (1-p0)*n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "afeats_size, bfeats_size, binary_size, dmfeats_size = (sample[\"atom_feats\"].shape[-1], sample[\"bond_feats\"].shape[-1],\n",
    "                                        sample[\"binary_feats\"].shape[-1], sample['domain_feats'].shape[-1])\n",
    "d1,d2,d3 = sample[\"binary_feats\"].shape\n",
    "binary_size= d3*d2\n",
    "\n",
    "\n",
    "test_batch_size = args.test_batch_size if args.test_batch_size is not None else args.batch_size\n",
    "su_dataloader = DataLoader(su_dataset, batch_size=test_batch_size, num_workers=args.num_workers, collate_fn=collate_fn)\n",
    "az_dataloader = DataLoader(az_dataset, batch_size=test_batch_size, num_workers=args.num_workers, collate_fn=collate_fn)\n",
    "dy_dataloader = DataLoader(dy_dataset, batch_size=test_batch_size, num_workers=args.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "net = RxnNet(depth=args.layers, dropout=args.dropout_rate, afeats_size=afeats_size, bfeats_size=bfeats_size,\n",
    "             hidden_size=args.hidden, binary_size=binary_size,dmfeats_size=dmfeats_size, use_domain=args.use_domain)\n",
    "trainer = RxnTrainer(net, lr=args.lr, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay,\n",
    "                     with_cuda=args.with_cuda, cuda_devices=args.cuda_devices, log_freq=args.log_freq,\n",
    "                     grad_clip=args.grad_clip, pos_weight=args.pos_weight, lr_decay=args.lr_decay,\n",
    "                     lr_steps=args.lr_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='output/dy_model_5.0.1-no_domain-11-200-1-100-0.01-0.9-10000-40/yield.model'\n",
    "#model = torch.nn.DataParallel(net)\n",
    "model= net\n",
    "betas=(args.adam_beta1,args.adam_beta2)\n",
    "optimizer = opt.Adam(model.parameters(), lr=args.lr, betas=betas, weight_decay=0)\n",
    "device = torch.device('cpu')\n",
    "#checkpoint = torch.load(PATH, map_location=device)\n",
    "#model.load_state_dict(checkpoint.state_dict())\n",
    "model = torch.load(PATH, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  wln\n",
      "isinstance(wln, nn.Module):  True\n",
      "=====\n",
      "name:  attention\n",
      "isinstance(attention, nn.Module):  True\n",
      "=====\n",
      "name:  yield_scoring\n",
      "isinstance(yield_scoring, nn.Module):  True\n",
      "=====\n",
      "name:  dropout\n",
      "isinstance(dropout, nn.Module):  True\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.named_children():\n",
    "    print('name: ', name)\n",
    "    print('isinstance({}, nn.Module): '.format(name), isinstance(child, nn.Module))\n",
    "    print('=====')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc1.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_nei.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_nei.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_atom.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_atom.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc2atom_nei.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc2bond_nei.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc2.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcapair.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcbinary.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcbinary.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcattention.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcattention.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fclocal.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcglobal.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcglobal.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcbinary.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcscore.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcscore.bias : True\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters(): \n",
    "    print('type(param): ', type(param))\n",
    "    print('isinstance(param, nn.Module): ', isinstance(param, nn.Module))\n",
    "    print('isinstance(param, nn.Parameter): ', isinstance(param, nn.Parameter))\n",
    "    print('isinstance(param, torch.Tensor) ', isinstance(param, torch.Tensor))\n",
    "    print(\"requires grad: \" , name, ':', param.requires_grad)\n",
    "    print('=====')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of YieldNet(\n",
       "  (wln): WLNet(\n",
       "    (fc1): Linear(in_features=29, out_features=200, bias=False)\n",
       "    (graph_conv_nei): Linear(in_features=206, out_features=200, bias=True)\n",
       "    (graph_conv_atom): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (fc2atom_nei): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (fc2bond_nei): Linear(in_features=6, out_features=200, bias=False)\n",
       "    (fc2): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (dropout): Dropout(p=0.04, inplace=False)\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (fcapair): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (fcbinary): Linear(in_features=15, out_features=200, bias=True)\n",
       "    (fcattention): Linear(in_features=200, out_features=1, bias=True)\n",
       "  )\n",
       "  (yield_scoring): YieldScoring(\n",
       "    (fclocal): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (fcglobal): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (fcbinary): Linear(in_features=225, out_features=200, bias=False)\n",
       "    (fcscore): Linear(in_features=200, out_features=1, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.04, inplace=False)\n",
       ")>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_conv_nei' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-495480caa36b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_conv_nei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mhook_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_conv_nei' is not defined"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "save_output = SaveOutput()\n",
    "\n",
    "hook_handles = []\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, graph_conv_nei):\n",
    "        handle = layer.register_forward_hook(save_output)\n",
    "        hook_handles.append(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save_output.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7507])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az_dataset[3]['yield_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n",
      "tensor([[0.1944]])\n"
     ]
    }
   ],
   "source": [
    "print(len(az_dataloader))\n",
    "model.eval()\n",
    "for i, data in enumerate(az_dataloader):\n",
    "    if i<=3:\n",
    "        if i==3:\n",
    "            mask_neis = torch.unsqueeze(data['n_bonds'].unsqueeze(-1) > torch.arange(0, 15, dtype=torch.int32).view(1, 1, -1), -1)\n",
    "            max_n_atoms = data['n_atoms'].max()\n",
    "            mask_atoms = torch.unsqueeze(data['n_atoms'].unsqueeze(-1) > torch.arange(0, max_n_atoms, dtype=torch.int32).view(1, -1),-1)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                yield_scores = model.forward(data['atom_feats'], data['bond_feats'],data['atom_graph'], \n",
    "                                                        data['bond_graph'], data['n_bonds'],data['n_atoms'], data['binary_feats'], \n",
    "                                                        mask_neis, mask_atoms,data['sparse_idx'],data['domain_feats'])\n",
    "                print(yield_scores)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1944]]), 18)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yield_scores,len(save_output.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "\n",
    "for i in range(len(save_output.outputs)):\n",
    "    np.save('data/image_'+str(i)+'.npy', save_output.outputs[i].detach().to('cpu').numpy())\n",
    "    #res.append(save_output.outputs[i].detach().to('cpu').numpy())\n",
    "#image= save_output.outputs.detach().to('cpu').numpy()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(model,optimizer,epoch, data_loader, train=True,valid=False):\n",
    "    avg_loss, tmp_r2 =0.0, 0\n",
    "    iters = len(data_loader)\n",
    "    n_samples = len(data_loader.dataset)\n",
    "    correct_yields , pred_yields =np.array([[0]]), np.array([[0]])\n",
    "    model.eval()\n",
    "    if True:\n",
    "        for i, data in enumerate(data_loader):\n",
    "            mask_neis = torch.unsqueeze(data['n_bonds'].unsqueeze(-1) > torch.arange(0, 15, dtype=torch.int32).view(1, 1, -1), -1)\n",
    "            max_n_atoms = data['n_atoms'].max()\n",
    "            mask_atoms = torch.unsqueeze(data['n_atoms'].unsqueeze(-1) > torch.arange(0, max_n_atoms, dtype=torch.int32).view(1, -1),-1)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                yield_scores = model.forward(data['atom_feats'], data['bond_feats'],data['atom_graph'], \n",
    "                                                    data['bond_graph'], data['n_bonds'],data['n_atoms'], data['binary_feats'], \n",
    "                                                    mask_neis, mask_atoms,data['sparse_idx'],data['domain_feats'])\n",
    "                criteria=nn.MSELoss()\n",
    "                loss= criteria(yield_scores, data['yield_label'])\n",
    "                loss = torch.mean(loss)\n",
    "                avg_loss += loss.item()\n",
    "                aa=data['yield_label'].cpu().detach().numpy()\n",
    "                bb=yield_scores.cpu().detach().numpy()\n",
    "                correct_yields=np.append(correct_yields,aa)\n",
    "                pred_yields=np.append(pred_yields,bb)\n",
    "\n",
    "        tmp_r2=r2_score(correct_yields,pred_yields)\n",
    "        logging.info(\"Epoch: {:2d}  Loss: {:f}  R2: {:6.2%} \".format(epoch,avg_loss, tmp_r2))\n",
    "        return correct_yields,pred_yields,tmp_r2,avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/21 11:35:08: Epoch:  1  Loss: 28.884723  R2: 91.86% \n"
     ]
    }
   ],
   "source": [
    "correct_yields,pred_yields, r2 ,loss= iterate( model,optimizer,1, dy_dataloader, train=False,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/21 11:40:26: Epoch:  1  Loss: 133.645681  R2: -103.15% \n"
     ]
    }
   ],
   "source": [
    "correct_yields,pred_yields, r2 ,loss= iterate( model,optimizer,1, az_dataloader, train=False,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/21 11:42:58: Epoch:  1  Loss: 872.601692  R2: -128.78% \n"
     ]
    }
   ],
   "source": [
    "correct_yields,pred_yields, r2 ,loss= iterate( model,optimizer,1, su_dataloader, train=False,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11045819235104222, 0.07012773506296444)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_yields),np.std(pred_yields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4178988746543945, 0.2872947228123882)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(correct_yields),np.std(correct_yields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([round(i,2) for i in list(pred_yields)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/'+args.output_name+'_y_true.txt', 'w') as f_true, open('data/'+args.output_name+'_y_pred.txt', 'w') as f_pred:\n",
    "    f_true.write(','.join(map(str, [float(n[0]) for n in a2])))\n",
    "    f_pred.write(','.join(map(str, [float(n[0]) for n in b2])))\n",
    "    \n",
    "    \n",
    "with open('data/'+args.output_name+'_train_scores.txt', 'w') as train_r2, open('data/'+args.output_name+'_test_scores.txt', 'w') as test_r2:\n",
    "    train_r2.write(','.join(map(str, [float(n) for n in train_scores])))\n",
    "    test_r2.write(','.join(map(str, [float(n) for n in test_scores])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score( data['yield_label'].cpu(), yield_scores.cpu().detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x300 at 0x7FB2F30072E8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit.Chaem import Draw\n",
    "Draw.MolToImage(mol)\n",
    "#Chem.FindMolChiralCenters(mol,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 0\n",
      "C 1\n",
      "C 2\n",
      "C 3\n",
      "C 4\n",
      "C 5\n",
      "C 6\n",
      "C 7\n",
      "Br 8\n"
     ]
    }
   ],
   "source": [
    "m = Chem.MolFromMolFile('data/doyle_reaction_mols/1-bromo-4-ethylbenzene.mol')\n",
    "m.GetNumAtoms()\n",
    "AllChem.Compute2DCoords(m)\n",
    "for atom in m.GetAtoms():\n",
    "    print(atom.GetSymbol(),atom.GetIdx())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_atom_mapping_doyle(mol_dir):\n",
    "    \n",
    "    smiles_mapping=defaultdict(dict) \n",
    "    \n",
    "    for mol_fn in glob.glob(mol_dir):\n",
    "        atoms , labels, atom_mapping =[], [], []\n",
    "        if 'methyl-isoxazole-5-carboxylate' in mol_fn:\n",
    "            print(mol_fn)\n",
    "            m = Chem.MolFromMolFile(mol_fn)\n",
    "            smiles=Chem.MolToSmiles(m)\n",
    "            mol_lines=open(mol_fn,'r').readlines()\n",
    "\n",
    "            for i,line in enumerate(mol_lines[4:]):\n",
    "                l=re.sub(' +', ' ', line.strip('\\n'))\n",
    "                l2=l.split(' ')\n",
    "                if len(l2)==17:\n",
    "                    atom=l2[4]\n",
    "                    if atom !='' and '*' not in atom and 'H' not in atom:\n",
    "                        atoms.append(atom)\n",
    "                if \"atom_labels\" in line:\n",
    "                    labels=[i.strip('*') for i in mol_lines[i+1+4].strip('\\n').split(' ') if ((i!='') and ('H' not in i))]\n",
    "                    print(labels)\n",
    "            if len(atoms)==len(labels):\n",
    "                for i in range(len(atoms)):\n",
    "                    atom_mapping.append((atoms[i],labels[i]))            \n",
    "                print(atom_mapping)\n",
    "            else:\n",
    "                print('Atoms and lables don\\'t match')\n",
    "                atom_mapping=[]\n",
    "\n",
    "            if smiles not in smiles_mapping:\n",
    "                print(mol_fn.split('/')[-1].split('.')[0])\n",
    "                smiles_mapping[mol_fn.split('/')[-1].split('.')[0]]=atom_mapping\n",
    "            else:\n",
    "                print(\"key exsists\")\n",
    "    return smiles_mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te\n"
     ]
    }
   ],
   "source": [
    "for mol_fn in glob.glob(\"data/doyle_reaction_mols/*.mol\"):\n",
    "    if 'methyl-isoxazole-5-carboxylate' in mol_fn:\n",
    "        print(\"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/doyle_reaction_mols/methyl-isoxazole-5-carboxylate.mol\n",
      "['O1', 'N1', 'C3', 'C4', 'C5', 'C1', 'O1', 'O2', 'C2']\n",
      "[('O', 'O1'), ('N', 'N1'), ('C', 'C3'), ('C', 'C4'), ('C', 'C5'), ('C', 'C1'), ('O', 'O1'), ('O', 'O2'), ('C', 'C2')]\n",
      "methyl-isoxazole-5-carboxylate\n"
     ]
    }
   ],
   "source": [
    "smiles_mapping=get_atom_mapping_doyle(\"data/doyle_reaction_mols/*.mol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 'O1'),\n",
       " ('N', 'N1'),\n",
       " ('C', 'C3'),\n",
       " ('C', 'C4'),\n",
       " ('C', 'C5'),\n",
       " ('C', 'C1'),\n",
       " ('O', 'O1'),\n",
       " ('O', 'O2'),\n",
       " ('C', 'C2')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_mapping['methyl-isoxazole-5-carboxylate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_mapping['methyl-isoxazole-5-carboxylate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 'C1'),\n",
       " ('C', 'C4'),\n",
       " ('C', 'C2'),\n",
       " ('C', 'C2'),\n",
       " ('C', 'C3'),\n",
       " ('C', 'C3'),\n",
       " ('C', 'C4'),\n",
       " ('C', 'C1'),\n",
       " ('Br', 'Br1')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_mapping['1-bromo-4-ethylbenzene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms=[{\"name\": \"C1\", \"atomic_num\": 6, \"partial_charge\": -0.025, \"nmr_shift\": 132.85}, {\"name\": \"C2\", \"atomic_num\": 6, \"partial_charge\": -0.058, \"nmr_shift\": 125.36}, {\"name\": \"C3\", \"atomic_num\": 6, \"partial_charge\": -0.211, \"nmr_shift\": 122.91}, {\"name\": \"C4\", \"atomic_num\": 6, \"partial_charge\": 0.167, \"nmr_shift\": 136.02}, {\"name\": \"H2\", \"atomic_num\": 1, \"partial_charge\": 0.104, \"nmr_shift\": 7.11}, {\"name\": \"H3\", \"atomic_num\": 1, \"partial_charge\": 0.14, \"nmr_shift\": 6.97}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_charge_dict=defaultdict(float)\n",
    "nmr_dict=defaultdict(float)\n",
    "for atom in atoms:\n",
    "    p_charge_dict[atom['name']]=atom['partial_charge']\n",
    "    nmr_dict[atom['name']]=atom['nmr_shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'C1': -0.025,\n",
       "             'C2': -0.058,\n",
       "             'C3': -0.211,\n",
       "             'C4': 0.167,\n",
       "             'H2': 0.104,\n",
       "             'H3': 0.14})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_charge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Chem.MolFromMolFile('data/doyle_reaction_mols/1-bromo-4-ethylbenzene.mol')\n",
    "partial_charges=[]\n",
    "nmr_shifts=[]\n",
    "\n",
    "for atom in m.GetAtoms():\n",
    "    atom_label=smiles_mapping['1-bromo-4-ethylbenzene'][atom.GetIdx()][1]\n",
    "    partial_charges.append(p_charge_dict[atom_label])\n",
    "    nmr_shifts.append(nmr_dict[atom_label])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.025, 0.167, -0.058, -0.058, -0.211, -0.211, 0.167, -0.025, 0.0]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[132.85, 136.02, 125.36, 125.36, 122.91, 122.91, 136.02, 132.85, 0.0]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmr_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=Chem.MolFromSmiles('CCc1ccc(Br)cc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem._ROAtomSeq at 0x7fd7fa850d50>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.GetAtoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 C\n",
      "1 C\n",
      "2 C\n",
      "3 C\n",
      "4 C\n",
      "5 C\n",
      "6 Br\n",
      "7 C\n",
      "8 C\n"
     ]
    }
   ],
   "source": [
    "for atom in mm.GetAtoms():\n",
    "    print(atom.GetIdx(),atom.GetSymbol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chem.MolFromMolFile('data/doyle_reaction_mols/1-bromo-4-ethylbenzene.mol')\n",
    "m.GetNumAtoms()\n",
    "Chem.MolToSmiles(m)\n",
    "#AllChem.Compute2DCoords(m2)\n",
    "#for atom in m.GetAtoms():\n",
    "    #print(atom.GetSymbol(),atom.GetIdx())\n",
    "#print(Chem.MolToMolBlock(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "file_name = args.train_dataset\n",
    "path = args.dataset_path\n",
    "rxns = []\n",
    "degree_codec = LabelEncoder()\n",
    "symbol_codec = LabelEncoder()\n",
    "expl_val_codec = LabelEncoder()\n",
    "bond_type_codec = LabelEncoder()\n",
    "max_nbonds = 10   \n",
    "symbols = set()\n",
    "degrees = set()\n",
    "explicit_valences = set()\n",
    "bond_types = set()\n",
    "\n",
    "a=True\n",
    "charges,shifts=[math.inf,-math.inf],[math.inf,-math.inf]\n",
    "with open(os.path.join(path, file_name)) as datafile:\n",
    "    data = json.load(datafile)\n",
    "    for line in data:\n",
    "\n",
    "        product=line['product']\n",
    "        reactants=line['reactants']\n",
    "        r_yield=line['yield']\n",
    "     \n",
    "        rxn = Rxn(product,reactants,r_yield)\n",
    "        mol = Chem.MolFromSmiles(rxn.reactants_smile)\n",
    "        #atom_idx = torch.tensor([atom.GetIdx()-1 for atom in mol.GetAtoms()], dtype=torch.int64)\n",
    "\n",
    "        mol_reactants=rxn.reactants\n",
    "        for mol_idx in range(1,len(mol_reactants)):\n",
    "            current_molecule=mol_reactants[mol_idx]\n",
    "            atoms=current_molecule.atoms\n",
    "            #p_charge_dict=defaultdict(float)\n",
    "            #nmr_dict=defaultdict(float)\n",
    "            for atom in atoms:\n",
    "                if 'partial_charge' in atom:\n",
    "                    charges[0],charges[1]= min(charges[0],atom['partial_charge']), max(charges[1],atom['partial_charge'])\n",
    "                if 'nmr_shift' in atom:\n",
    "                    shifts[0],shifts[1]= min(shifts[0],atom['nmr_shift']), max(shifts[1],atom['nmr_shift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.96, 1.858]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.41, 168.89]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yieldrxn",
   "language": "python",
   "name": "yieldrxn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
