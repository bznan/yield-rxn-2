{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script load a given model and generates \n",
    "model analysis. First, it prints basic stats of \n",
    "how the model over or under predicts.\n",
    "The it outputs the actual vs predicted yields\n",
    "alsong with the given smiles for each sample\n",
    "It also generates the GNN activations for each\n",
    "input and plots the molecule with highly activated \n",
    "areas.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /afs/crc.nd.edu/user/m/msaebi/Public/chemistry/yield_rxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import rdkit\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import DrawingOptions\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem.Draw import DrawingOptions\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset,Subset\n",
    "\n",
    "from rxntorch.containers.reaction import Rxn\n",
    "from rxntorch.containers.dataset import RxnGraphDataset as RxnGD\n",
    "from rxntorch.utils import collate_fn\n",
    "from rxntorch.models.yield_network import YieldNet, YieldTrainer\n",
    "from rxntorch.models import yield_network\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score,r2_score\n",
    "\n",
    "import scripts.load_utils as lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--abs'], dest='abs', nargs=None, const=None, default='abs', type=<class 'str'>, choices=None, help='Take the average over aboslute/no absolute/sigmoid/relu value of predicted yield', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"-p\", \"--dataset_path\", type=str, default='./data/', help=\"train dataset\")\n",
    "parser.add_argument(\"-dn\", \"--dataset_name\", required=True, type=str, help=\"dataset name. Options: az (AstraZeneca),dy (Doyle),su (Suzuki)\")\n",
    "parser.add_argument(\"-op\", \"--output_path\", type=str, default='./output/', help=\"saved model path\")\n",
    "parser.add_argument(\"-o\", \"--output_name\", required=True, type=str, help=\"e.g. rxntorch.model\")\n",
    "parser.add_argument(\"-sn\", \"--split_set_num\", type=int, default=1, help=\"Choose one split set for train and test. Options: 1-10\")\n",
    "\n",
    "parser.add_argument(\"-dr\", \"--dropout_rate\", type=float, default=0.04, help=\"Ratio of samples to reserve for valid data\")\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, default=40, help=\"number of batch_size\")\n",
    "parser.add_argument(\"-tb\", \"--test_batch_size\", type=int, default=1, help=\"batch size for evaluation\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=200, help=\"number of epochs\")\n",
    "parser.add_argument(\"-hs\", \"--hidden\", type=int, default=200, help=\"hidden size of model layers\")\n",
    "parser.add_argument(\"-l\", \"--layers\", type=int, default=2, help=\"number of layers\")\n",
    "\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-2, help=\"learning rate of the optimizer\")\n",
    "parser.add_argument(\"-lrd\", \"--lr_decay\", type=float, default=0.5, help=\"Decay factor for reducing the learning rate\")\n",
    "parser.add_argument(\"-lrs\", \"--lr_steps\", type=int, default=10000,help=\"Number of steps between learning rate decay\")\n",
    "\n",
    "parser.add_argument(\"-awd\",\"--adam_weight_decay\", type=float, default=0.0, help=\"weight_decay of adam\")\n",
    "parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"adam first beta value\")\n",
    "parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"adam second beta value\")\n",
    "\n",
    "parser.add_argument(\"-gc\", \"--grad_clip\", type=float, default=None, help=\"value for gradient clipping\")\n",
    "parser.add_argument(\"-pw\", \"--pos_weight\", type=float, default=None, help=\"Weights positive samples for imbalance\")\n",
    "\n",
    "parser.add_argument(\"-w\", \"--num_workers\", type=int, default=4, help=\"dataloader worker size\")\n",
    "parser.add_argument(\"--with_cuda\", type=bool, default=True, help=\"training with CUDA: true, or false\")\n",
    "parser.add_argument(\"--cuda_devices\", type=int, nargs='*', default=None, help=\"CUDA device ids\")\n",
    "\n",
    "parser.add_argument(\"--log_freq\", type=int, default=100, help=\"printing loss every n iter: setting n\")\n",
    "parser.add_argument(\"--seed\", type=int, default=0, help=\"random seed\")\n",
    "parser.add_argument(\"-ud\",\"--use_domain\", type=str, required=True, help=\"use domain features or not. options: rdkit: combination od rdkit feature and bozhao features. no_rdkit: only bozhao features. no_domain: neither.\")\n",
    "parser.add_argument(\"-mb\",\"--max_nbonds\", type=int, default=15, help=\"maximum number of bonds for binary features\")\n",
    "parser.add_argument(\"-ma\",\"--max_natoms\", type=int, default=15, help=\"maximum number of atoms for binary features\")\n",
    "parser.add_argument(\"--abs\", type=str, default='abs', help=\"Take the average over aboslute/no absolute/sigmoid/relu value of predicted yield\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "args = parser.parse_args(args=[\"-dn\", \"az\",\n",
    "                               \"--batch_size\",'40',\n",
    "                              \"-o\" ,\"az_model_5.3.0-learn-w\",\n",
    "                              \"--use_domain\",\"rdkit\",\n",
    "                              \"-gc\",\"0.8\",\n",
    "                               \"--layers\",'2',\n",
    "                               \"--hidden\",'20',\n",
    "                               \"--epochs\",'100',\n",
    "                               \"-sn\" ,\"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from:\n",
      "  output/az_model_5.3.0-learn-w-gc-rdkit-abs-set-2-20-2-100-0.01-0.5-10000-40\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed )\n",
    "torch.cuda.manual_seed_all(args.seed )\n",
    "\n",
    "#input specs\n",
    "data_type=args.dataset_name\n",
    "gc= 'gc' if args.grad_clip else ''\n",
    "Abs=args.abs \n",
    "\n",
    "model_name = '-'.join(map(str,[args.output_name, gc, args.use_domain, Abs, 'set',args.split_set_num,\n",
    "                               args.hidden, args.layers, args.epochs, args.lr, args.lr_decay, \n",
    "                               args.lr_steps,args.batch_size]))\n",
    "\n",
    "model_path= os.path.join('output',model_name)\n",
    "if not os.path.exists(model_path):\n",
    "    raise Exception('model \\n'+model_path+'\\ndoesn\\'t exsist')\n",
    "    \n",
    "else:    \n",
    "    print(\"Model loaded from:\\n \",model_path)\n",
    "    \n",
    "#if use_domain=no_domain, just load either rdkit or no_rdkit .csv file and the\n",
    "#set domain features to 0.\n",
    "\n",
    "ext= '_'+args.use_domain if 'rdkit' in args.use_domain else '_no_rdkit' \n",
    "data_path = os.path.join(args.dataset_path,data_type)\n",
    "processed_path = os.path.join(data_path,'processed')\n",
    "\n",
    "input_split_idx_file = os.path.join(processed_path,'train_test_idxs.pickle')\n",
    "processed_data_file = os.path.join(processed_path,''.join([data_type, ext,'.csv']))\n",
    "selected_features_fn = os.path.join(data_path,'rf_results','selected_feats.txt')\n",
    "\n",
    "#output specs\n",
    "# Saves model predictions and summary to the model_res folder.\n",
    "model_res= os.path.join('output/'+model_name,'model_res')\n",
    "model_preds_test_fn = os.path.join(model_res,'model_preds_test.csv')\n",
    "model_preds_train_fn = os.path.join(model_res,'model_preds_train.csv')\n",
    "model_summary_fn= os.path.join(model_res,'model_summary.pickle')\n",
    "\n",
    "if not os.path.exists(model_res):\n",
    "    os.mkdir(model_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset in ./data/az/processed/az_rdkit.csv\n",
      "Using the split set number 2\n",
      "Number of all available features: 1218\n",
      "Selecting features...\n",
      "Number of features after feature selection: 813\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#load_train_test sets\n",
    "#################################################################\n",
    "split_set_num= args.split_set_num\n",
    "\n",
    "with open(input_split_idx_file, 'rb') as handle:\n",
    "    idx_dict = pickle.load(handle)\n",
    "    \n",
    "selected_features = open(selected_features_fn,'r').readlines()[0].split(',')\n",
    "\n",
    "print(\"Loading Dataset in {dataset}\".format( dataset=processed_data_file))\n",
    "print(\"Using the split set number {split}\".format( split=split_set_num))\n",
    "\n",
    "df=pd.read_csv(processed_data_file,index_col=0)\n",
    "train_set= df.iloc[idx_dict['train_idx'][split_set_num]]\n",
    "test_set = df.iloc[idx_dict['test_idx'][split_set_num]]\n",
    "\n",
    "smiles_feature_names = [\"id\",\"yield\",\"reactant_smiles\",\"solvent_smiles\",\"base_smiles\",\"product_smiles\"]\n",
    "domain_feature_names = [f for f in df.columns if f not in smiles_feature_names]\n",
    "\n",
    "#apply feature selection\n",
    "print(\"Number of all available features: {num}\".format(num=len(domain_feature_names)))\n",
    "if args.use_domain=='rdkit':\n",
    "    domain_feature_names = [f for f in domain_feature_names if f in selected_features]\n",
    "    print(\"Selecting features...\")\n",
    "    print(\"Number of features after feature selection: {num}\".format(num=len(domain_feature_names)))\n",
    "else:\n",
    "    print(\"Not running feature selection!\")\n",
    "\n",
    "\n",
    "train_set_domain = train_set[domain_feature_names]\n",
    "test_set_domain = test_set[domain_feature_names]\n",
    "train_set_smiles = train_set[smiles_feature_names]\n",
    "test_set_smiles = test_set[smiles_feature_names]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_set_domain_scaled = pd.DataFrame(scaler.fit_transform(train_set_domain),columns = domain_feature_names)\n",
    "test_set_domain_scaled = pd.DataFrame(scaler.transform(test_set_domain),columns = domain_feature_names)\n",
    "\n",
    "assert train_set_domain.shape[0]  == train_set_smiles.shape[0] == train_set_domain_scaled.shape[0]\n",
    "assert test_set_domain.shape[0]  == test_set_smiles.shape[0] == test_set_domain_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------Dataset-------------------------------------\n",
      "450 samples for training ,300 samples for testing\n",
      "-------------------------------------Model--------------------------------------\n",
      "Graph convolution layers: 2  Hidden size: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"{:-^80}\".format(\"Dataset\"))\n",
    "#feeding train smiles to test and vice versa to make sure our encoding is consistent.\n",
    "# no label information is used on test set here.\n",
    "train_dataset = RxnGD(train_set_domain_scaled,train_set_smiles, test_set_smiles, args.max_nbonds, args.max_natoms, args.use_domain)\n",
    "test_dataset = RxnGD(test_set_domain_scaled, test_set_smiles, train_set_smiles, args.max_nbonds, args.max_natoms, args.use_domain)\n",
    "\n",
    "                   \n",
    "sample = train_dataset[3]\n",
    "afeats_size, bfeats_size, binary_size, dmfeats_size = (sample[\"atom_feats\"].shape[-1], sample[\"bond_feats\"].shape[-1],\n",
    "                                        sample[\"binary_feats\"].shape[-1], sample['domain_feats'].shape[-1])\n",
    "d1,d2,d3 = sample[\"binary_feats\"].shape\n",
    "binary_size= d3*d2\n",
    "\n",
    "print(\"{:d} samples for training ,{:d} samples for testing\".format(train_set.shape[0], test_set.shape[0]))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.test_batch_size, num_workers=args.num_workers, shuffle=True,\n",
    "                              collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.test_batch_size, num_workers=args.num_workers, \n",
    "                             collate_fn=collate_fn,drop_last=True)\n",
    "\n",
    "\n",
    "print(\"{:-^80}\".format(\"Model\"))\n",
    "print(\"Graph convolution layers: {}  Hidden size: {}\".format(\n",
    "    args.layers, args.hidden, args.batch_size, args.epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas=(args.adam_beta1,args.adam_beta2)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = torch.load( os.path.join('output/'+model_name+'/yield.model'), map_location=device)\n",
    "optimizer = opt.Adam(model.parameters(), lr=args.lr, betas=betas, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test set analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "R2 score:  0.0026905636568043123\n",
      "Num low-yield:  249\n",
      "Num high-yield:  51\n",
      "Percentage of lows --> high: 7.63%\n",
      "Percentage of lows --> low: 92.37%\n",
      "Percentage of highs --> high: 15.69%\n",
      "Percentage of highs --> low: 84.31%\n",
      "\n",
      "Writing actual and predicted yield values for each sample...\n",
      "\n",
      "Generating activation figures...\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set\")\n",
    "correct_yields_test, pred_yields_test, data_dict_test = lu.get_model_outputs(model,test_dataloader,test_set_smiles)\n",
    "lu.get_basic_stats(data_dict_test)\n",
    "lu.write_model_preds(model_preds_test_fn,data_dict_test)\n",
    "lu.plot_activation_all_data(data_dict_test,'test',model_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Train set analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set\")\n",
    "correct_yields_train, pred_yields_train, data_dict_train= get_model_outputs(model,train_dataloader,train_set_smiles)\n",
    "lu.get_basic_stats(data_dict_train)\n",
    "lu.write_model_preds(model_preds_train_fn,data_dict_train)\n",
    "lu.plot_activation_all_data(data_dict_train,'train',model_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yieldrxn",
   "language": "python",
   "name": "yieldrxn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
