{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import json\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit\n",
    "from rxntorch.containers.reaction import Rxn\n",
    "from rxntorch.containers.molecule import Mol\n",
    "from rxntorch.containers.dataset import RxnGraphDataset as RxnGD\n",
    "from rxntorch.utils import collate_fn\n",
    "from rxntorch.models.yield_network import YieldNet as RxnNet, YieldTrainer as RxnTrainer\n",
    "from rxntorch.models import yield_network\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import DrawingOptions\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score,r2_score\n",
    "\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-ud', '--use_domain'], dest='use_domain', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='use domain features or not', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"-p\", \"--dataset_path\", type=str, default='./data/', help=\"train dataset\")\n",
    "parser.add_argument(\"-mp\",\"--mol_path\", type=str, default='doyle_reaction_mols', help=\"path to mol files\")\n",
    "\n",
    "parser.add_argument(\"-c\", \"--train_dataset\", required=True, type=str, help=\"train dataset\")\n",
    "parser.add_argument(\"-t\", \"--test_dataset\", type=str, default=None, help=\"test set\")\n",
    "parser.add_argument(\"-op\", \"--output_path\", type=str, default='./output/', help=\"saved model path\")\n",
    "parser.add_argument(\"-o\", \"--output_name\", required=True, type=str, help=\"e.g. rxntorch.model\")\n",
    "parser.add_argument(\"-ds\", \"--train_split\", type=float, default=0.7, help=\"Ratio of samples to reserve for test data\")\n",
    "parser.add_argument(\"-vs\", \"--valid_split\", type=float, default=0.333, help=\"Ratio of samples to reserve for valid data\")\n",
    "parser.add_argument(\"-dr\", \"--dropout_rate\", type=float, default=0.333, help=\"Ratio of samples to reserve for valid data\")\n",
    "\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1, help=\"number of batch_size\")\n",
    "parser.add_argument(\"-tb\", \"--test_batch_size\", type=int, default=None, help=\"batch size for evaluation\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, help=\"number of epochs\")\n",
    "parser.add_argument(\"-hs\", \"--hidden\", type=int, default=200, help=\"hidden size of model layers\")\n",
    "parser.add_argument(\"-l\", \"--layers\", type=int, default=3, help=\"number of layers\")\n",
    "\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-2, help=\"learning rate of the optimizer\")\n",
    "parser.add_argument(\"-lrd\", \"--lr_decay\", type=float, default=0.9,\n",
    "                    help=\"Decay factor for reducing the learning rate\")\n",
    "parser.add_argument(\"-lrs\", \"--lr_steps\", type=int, default=10000,\n",
    "                    help=\"Number of steps between learning rate decay\")\n",
    "parser.add_argument(\"-awd\",\"--adam_weight_decay\", type=float, default=0.0, help=\"weight_decay of adam\")\n",
    "parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"adam first beta value\")\n",
    "parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"adam second beta value\")\n",
    "parser.add_argument(\"-gc\", \"--grad_clip\", type=float, default=None, help=\"value for gradient clipping\")\n",
    "parser.add_argument(\"-pw\", \"--pos_weight\", type=float, default=None, help=\"Weights positive samples for imbalance\")\n",
    "\n",
    "parser.add_argument(\"-w\", \"--num_workers\", type=int, default=4, help=\"dataloader worker size\")\n",
    "parser.add_argument(\"--with_cuda\", type=bool, default=True, help=\"training with CUDA: true, or false\")\n",
    "parser.add_argument(\"--cuda_devices\", type=int, nargs='*', default=None, help=\"CUDA device ids\")\n",
    "\n",
    "parser.add_argument(\"--log_freq\", type=int, default=50, help=\"printing loss every n iter: setting n\")\n",
    "parser.add_argument(\"--seed\", type=int, default=12, help=\"random seed\")\n",
    "parser.add_argument(\"-ud\",\"--use_domain\", type=str, required='True', help=\"use domain features or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([87, 20]),\n",
       " torch.Size([89, 5]),\n",
       " torch.Size([87, 15]),\n",
       " torch.Size([87, 15]),\n",
       " torch.Size([1, 224]),\n",
       " torch.Size([15, 15, 15]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#args = parser.parse_args()\n",
    "data_type='dy'\n",
    "args = parser.parse_args(args=['-p','data/','-c', data_type+'_reactions_data.json', '-o', 'model.01','-ud', 'False', '-mp', data_type+'_reaction_mols'])\n",
    "random_seed=args.seed\n",
    "dataset = RxnGD(data_type+'/'+data_type+'_reactions_data.json',path=args.dataset_path)\n",
    "sample = dataset[3]\n",
    "sample['atom_feats'].shape, sample['bond_feats'].shape, sample['atom_graph'].shape,sample['bond_graph'].shape,sample['domain_feats'].shape,sample[\"binary_feats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "afeats_size, bfeats_size, binary_size, dmfeats_size = (sample[\"atom_feats\"].shape[-1], sample[\"bond_feats\"].shape[-1],\n",
    "                                        sample[\"binary_feats\"].shape[-1], sample['domain_feats'].shape[-1])\n",
    "d1,d2,d3 = sample[\"binary_feats\"].shape\n",
    "binary_size= d3*d2\n",
    "\n",
    "test_batch_size = args.test_batch_size if args.test_batch_size is not None else args.batch_size\n",
    "dataloader = DataLoader(dataset, batch_size=test_batch_size, num_workers=args.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "net = RxnNet(depth=args.layers, dropout=args.dropout_rate, afeats_size=afeats_size, bfeats_size=bfeats_size,\n",
    "             hidden_size=args.hidden, binary_size=binary_size,dmfeats_size=dmfeats_size, use_domain=args.use_domain)\n",
    "trainer = RxnTrainer(net, lr=args.lr, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay,\n",
    "                     with_cuda=args.with_cuda, cuda_devices=args.cuda_devices, log_freq=args.log_freq,\n",
    "                     grad_clip=args.grad_clip, pos_weight=args.pos_weight, lr_decay=args.lr_decay,\n",
    "                     lr_steps=args.lr_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['-p','data/','-c', data_type+'_reactions_data.json', '-o', 'model.01','-ud', 'False', '-mp', data_type+'_reaction_mols'])\n",
    "random_seed=args.seed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_smiles_from_raw_json(json):\n",
    "    reactant_smiles= '.'.join([reactant['smiles'] for reactant in json['reactants']])\n",
    "    if 'solvent' in json.keys():\n",
    "        solvent_smiles = json['solvent'][0]\n",
    "    else:\n",
    "        solvent_smiles = 'CS(=O)C'\n",
    "    \n",
    "    base_smiles= json['base']['smiles']\n",
    "    rxn_smiles= reactant_smiles+'.'+ solvent_smiles+'.'+base_smiles\n",
    "    return rxn_smiles\n",
    "\n",
    "def get_component_idx(mol):\n",
    "    comp = {}\n",
    "    symbols = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "    all_atoms=set(symbols)\n",
    "    n_atoms=len(all_atoms)\n",
    "    cumulative_idx=0\n",
    "    for i, s in enumerate(smiles.split('.')):\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        for atom in mol.GetAtoms():\n",
    "            comp[cumulative_idx-1] = i #this approach in getting cumulative indx works and it aligns \n",
    "                                        # with vieweing all mols in one mol and then getting the atom idxs\n",
    "            cumulative_idx +=1      # bases don't have bonds!!!\n",
    "    return comp\n",
    "\n",
    "def get_bond_map(mol):\n",
    "    bond_map=defaultdict()\n",
    "    for bond in mol.GetBonds():\n",
    "        a1 = bond.GetBeginAtom().GetIdx()\n",
    "        a2 = bond.GetEndAtom().GetIdx()\n",
    "        bond_map[(a1,a2)] = bond.GetIdx()\n",
    "        bond_map[(a2,a1)] = bond.GetIdx()\n",
    "        #print(a1,a2,\"bond: \",bond.GetIdx())\n",
    "    return bond_map\n",
    "\n",
    "def get_top_activated(gnn_weights, threshold,bond_map):\n",
    "    most_activated_atoms=set()\n",
    "    most_activated_bonds=set()\n",
    "    for i in range(gnn_weights.shape[0]):\n",
    "        for j in range(gnn_weights.shape[1]):\n",
    "            if abs(gnn_weights[i][j])>threshold:\n",
    "                most_activated_atoms.add(i)\n",
    "                most_activated_atoms.add(j)\n",
    "                #if (i,j) in bond_map:\n",
    "                #most_activated_bonds.add(bond_map[(i,j)])\n",
    "                    \n",
    "    return most_activated_atoms, most_activated_bonds\n",
    "\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem.Draw import DrawingOptions\n",
    " \n",
    "\n",
    "DrawingOptions.atomLabelFontSize = 55\n",
    "DrawingOptions.dotsPerAngstrom = 100\n",
    "DrawingOptions.bondLineWidth = 3.0\n",
    "def moltosvg(mol,highlightAtoms,output,molSize=(600,600),kekulize=True):\n",
    "    mc = Chem.Mol(mol.ToBinary())\n",
    "    if kekulize:\n",
    "        try:\n",
    "            Chem.Kekulize(mc)\n",
    "        except:\n",
    "            mc = Chem.Mol(mol.ToBinary())\n",
    "    if not mc.GetNumConformers():\n",
    "        rdDepictor.Compute2DCoords(mc)\n",
    "    drawer = rdMolDraw2D.MolDraw2DSVG(molSize[0],molSize[1])\n",
    "    drawer.DrawMolecule(mc,highlightAtoms)\n",
    "    drawer.FinishDrawing()\n",
    "    svg = drawer.GetDrawingText()\n",
    "    \n",
    "    d2d = Draw.MolDraw2DCairo(molSize[0],molSize[1])\n",
    "    d2d.DrawMolecule(mc,highlightAtoms)\n",
    "    d2d.FinishDrawing()\n",
    "    png_data = d2d.GetDrawingText()\n",
    "\n",
    "    # save png to file\n",
    "    with open(output, 'wb') as png_file:\n",
    "        png_file.write(png_data)\n",
    "    return svg.replace('svg:','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='output//yield.model'\n",
    "model_name='dy_models/dy_model_5.0.1-no_domain-11-200-1-100-0.01-0.9-10000-40'\n",
    "model_name= 'su_model_5.2.0-rdkt-no-abs-w_domain-20-200-2-200-0.01-0.5-10000-40'\n",
    "model_name = 'dy_model_4.6.3-noabs-w_domain-11-200-2-200-0.01-0.5-10000-40'\n",
    "model_name= 'dy_model_4.6.3-noabs-w_domain-15-200-2-200-0.01-0.5-10000-40'\n",
    "\n",
    "\n",
    "PATH='output/'+model_name+'/yield.model'\n",
    "betas=(args.adam_beta1,args.adam_beta2)\n",
    "device = torch.device('cpu')\n",
    "model = torch.load(PATH, map_location=device)\n",
    "optimizer = opt.Adam(model.parameters(), lr=args.lr, betas=betas, weight_decay=0)\n",
    "\n",
    "with open('data/'+data_type+'/'+data_type+'_reactions_data.json') as datafile:\n",
    "    raw_data = json.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  wln\n",
      "isinstance(wln, nn.Module):  True\n",
      "=====\n",
      "name:  attention\n",
      "isinstance(attention, nn.Module):  True\n",
      "=====\n",
      "name:  yield_scoring\n",
      "isinstance(yield_scoring, nn.Module):  True\n",
      "=====\n",
      "name:  dropout\n",
      "isinstance(dropout, nn.Module):  True\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.named_children():\n",
    "    print('name: ', name)\n",
    "    print('isinstance({}, nn.Module): '.format(name), isinstance(child, nn.Module))\n",
    "    print('=====')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc1.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_nei.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_nei.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_atom.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.graph_conv_atom.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc2atom_nei.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc2bond_nei.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  wln.fc2.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcapair.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcbinary.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcbinary.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcattention.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  attention.fcattention.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fclocal.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcglobal.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcglobal.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcbinary.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcscore.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.fcscore.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.dcscore.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.dcscore.bias : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.domain.weight : True\n",
      "=====\n",
      "type(param):  <class 'torch.nn.parameter.Parameter'>\n",
      "isinstance(param, nn.Module):  False\n",
      "isinstance(param, nn.Parameter):  True\n",
      "isinstance(param, torch.Tensor)  True\n",
      "requires grad:  yield_scoring.domain.bias : True\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters(): \n",
    "    print('type(param): ', type(param))\n",
    "    print('isinstance(param, nn.Module): ', isinstance(param, nn.Module))\n",
    "    print('isinstance(param, nn.Parameter): ', isinstance(param, nn.Parameter))\n",
    "    print('isinstance(param, torch.Tensor) ', isinstance(param, torch.Tensor))\n",
    "    print(\"requires grad: \" , name, ':', param.requires_grad)\n",
    "    print('=====')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WLNet(\n",
       "  (fc1): Linear(in_features=29, out_features=200, bias=False)\n",
       "  (graph_conv_nei): Linear(in_features=206, out_features=200, bias=True)\n",
       "  (graph_conv_atom): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (fc2atom_nei): Linear(in_features=200, out_features=200, bias=False)\n",
       "  (fc2bond_nei): Linear(in_features=6, out_features=200, bias=False)\n",
       "  (fc2): Linear(in_features=200, out_features=200, bias=False)\n",
       "  (dropout): Dropout(p=0.04, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wln.fc1.weight torch.Size([200, 29])\n",
      "wln.graph_conv_nei.weight torch.Size([200, 206])\n",
      "wln.graph_conv_nei.bias torch.Size([200])\n",
      "wln.graph_conv_atom.weight torch.Size([200, 400])\n",
      "wln.graph_conv_atom.bias torch.Size([200])\n",
      "wln.fc2atom_nei.weight torch.Size([200, 200])\n",
      "wln.fc2bond_nei.weight torch.Size([200, 6])\n",
      "wln.fc2.weight torch.Size([200, 200])\n",
      "attention.fcapair.weight torch.Size([200, 200])\n",
      "attention.fcbinary.weight torch.Size([200, 15])\n",
      "attention.fcbinary.bias torch.Size([200])\n",
      "attention.fcattention.weight torch.Size([1, 200])\n",
      "attention.fcattention.bias torch.Size([1])\n",
      "yield_scoring.fclocal.weight torch.Size([200, 200])\n",
      "yield_scoring.fcglobal.weight torch.Size([200, 200])\n",
      "yield_scoring.fcglobal.bias torch.Size([200])\n",
      "yield_scoring.fcbinary.weight torch.Size([200, 225])\n",
      "yield_scoring.fcscore.weight torch.Size([1, 200])\n",
      "yield_scoring.fcscore.bias torch.Size([1])\n",
      "yield_scoring.dcscore.weight torch.Size([1, 200])\n",
      "yield_scoring.dcscore.bias torch.Size([1])\n",
      "yield_scoring.domain.weight torch.Size([200, 634])\n",
      "yield_scoring.domain.bias torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters(): \n",
    "    print(name,param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 YieldNet(\n",
      "  (wln): WLNet(\n",
      "    (fc1): Linear(in_features=29, out_features=200, bias=False)\n",
      "    (graph_conv_nei): Linear(in_features=206, out_features=200, bias=True)\n",
      "    (graph_conv_atom): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (fc2atom_nei): Linear(in_features=200, out_features=200, bias=False)\n",
      "    (fc2bond_nei): Linear(in_features=6, out_features=200, bias=False)\n",
      "    (fc2): Linear(in_features=200, out_features=200, bias=False)\n",
      "    (dropout): Dropout(p=0.04, inplace=False)\n",
      "  )\n",
      "  (attention): Attention(\n",
      "    (fcapair): Linear(in_features=200, out_features=200, bias=False)\n",
      "    (fcbinary): Linear(in_features=15, out_features=200, bias=True)\n",
      "    (fcattention): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      "  (yield_scoring): YieldScoring(\n",
      "    (fclocal): Linear(in_features=200, out_features=200, bias=False)\n",
      "    (fcglobal): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (fcbinary): Linear(in_features=225, out_features=200, bias=False)\n",
      "    (fcscore): Linear(in_features=200, out_features=1, bias=True)\n",
      "    (dcscore): Linear(in_features=200, out_features=1, bias=True)\n",
      "    (domain): Linear(in_features=634, out_features=200, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.04, inplace=False)\n",
      ")\n",
      "2 WLNet(\n",
      "  (fc1): Linear(in_features=29, out_features=200, bias=False)\n",
      "  (graph_conv_nei): Linear(in_features=206, out_features=200, bias=True)\n",
      "  (graph_conv_atom): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (fc2atom_nei): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (fc2bond_nei): Linear(in_features=6, out_features=200, bias=False)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (dropout): Dropout(p=0.04, inplace=False)\n",
      ")\n",
      "3 Linear(in_features=29, out_features=200, bias=False)\n",
      "4 Linear(in_features=206, out_features=200, bias=True)\n",
      "5 Linear(in_features=400, out_features=200, bias=True)\n",
      "6 Linear(in_features=200, out_features=200, bias=False)\n",
      "7 Linear(in_features=6, out_features=200, bias=False)\n",
      "8 Linear(in_features=200, out_features=200, bias=False)\n",
      "9 Dropout(p=0.04, inplace=False)\n",
      "10 Attention(\n",
      "  (fcapair): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (fcbinary): Linear(in_features=15, out_features=200, bias=True)\n",
      "  (fcattention): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "11 Linear(in_features=200, out_features=200, bias=False)\n",
      "12 Linear(in_features=15, out_features=200, bias=True)\n",
      "13 Linear(in_features=200, out_features=1, bias=True)\n",
      "14 YieldScoring(\n",
      "  (fclocal): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (fcglobal): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fcbinary): Linear(in_features=225, out_features=200, bias=False)\n",
      "  (fcscore): Linear(in_features=200, out_features=1, bias=True)\n",
      "  (dcscore): Linear(in_features=200, out_features=1, bias=True)\n",
      "  (domain): Linear(in_features=634, out_features=200, bias=True)\n",
      ")\n",
      "15 Linear(in_features=200, out_features=200, bias=False)\n",
      "16 Linear(in_features=200, out_features=200, bias=True)\n",
      "17 Linear(in_features=225, out_features=200, bias=False)\n",
      "18 Linear(in_features=200, out_features=1, bias=True)\n",
      "19 Linear(in_features=200, out_features=1, bias=True)\n",
      "20 Linear(in_features=634, out_features=200, bias=True)\n",
      "21 Dropout(p=0.04, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for layer in model.modules():\n",
    "    i+=1\n",
    "    print(i,layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 19, 52, 61, 63, 74, 83, 85, 96, 138, 140, 146, 148, 149, 151, 162, 309, 310, 313, 314, 316, 319, 320, 321, 322, 324, 332, 335, 346, 354, 357, 363, 364, 365, 366, 368, 369, 374, 375, 376, 377, 379, 380, 385, 386, 387, 390, 396, 397, 398, 401, 434, 440, 441, 442, 445, 451, 452, 453, 454, 456, 467, 474, 485, 489, 507, 511, 517, 518, 539, 540, 542, 543, 544, 545, 547, 551, 552, 553, 554, 555, 556, 558, 565, 566, 572, 573, 575, 576, 577, 578, 580, 583, 584, 586, 587, 588, 589, 591, 594, 595, 597, 598, 599, 600, 602, 606, 609, 610, 617, 618, 619, 621, 622, 623, 624, 628, 629, 630, 632, 639, 640, 643, 650, 651, 654, 661, 662, 663, 665, 666, 668, 669, 672, 673, 674, 675, 676, 677, 678, 679, 683, 684, 685, 687, 688, 690, 691, 694, 696, 699, 701, 705, 707, 710, 712, 718, 738, 745, 749, 754, 756, 760, 765, 767, 782, 784, 790, 793, 795, 797, 800, 801, 804, 815, 826, 848, 850, 853, 855, 856, 859, 861, 864, 870, 872, 874, 881, 883, 886, 888, 889, 892, 894, 897, 899, 900, 903, 905, 908, 910, 911, 914, 916, 918, 922, 924, 925, 926, 927, 928, 929, 930, 931, 932, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 946, 947, 948, 949, 950, 951, 952, 953, 955, 957, 958, 959, 960, 962, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 990, 991, 992, 993, 994, 995, 996, 997, 999]\n"
     ]
    }
   ],
   "source": [
    "dataset[sample_num]['yield_label']\n",
    "print(high_yield_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "def plot_activations(gnn_weights,sample_num,actual,predicted,good_bad):\n",
    "    \n",
    "    threshold= round(np.quantile(np.abs(gnn_weights),0.99),2)\n",
    "    most_activated_atoms, most_activated_bonds = get_top_activated(gnn_weights, threshold,bond_map)\n",
    "    fig_path='vis/'+model_name+'/'+good_bad+'/'\n",
    "    image_path=fig_path+str(sample_num)+'_Y_'+str(actual)+'_Pred_'+str(predicted)+'_'+str(threshold)+'.png'\n",
    "    if not os.path.exists(fig_path):\n",
    "        os.mkdir(fig_path)\n",
    "    SVG(moltosvg(mol,most_activated_atoms,image_path,molSize=(600,600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888 yield label raw: 0.01\n",
      "predicted yield: -0.04, yield label: 0.01\n",
      "889 yield label raw: 0.13\n",
      "predicted yield: 0.08, yield label: 0.13\n",
      "891 yield label raw: 0.05\n",
      "predicted yield: 0.08, yield label: 0.05\n",
      "894 yield label raw: 0.82\n",
      "predicted yield: 1.07, yield label: 0.82\n",
      "895 yield label raw: 0.9\n",
      "predicted yield: 0.99, yield label: 0.9\n",
      "896 yield label raw: 0.94\n",
      "predicted yield: 0.93, yield label: 0.94\n",
      "900 yield label raw: 0.09\n",
      "predicted yield: 0.12, yield label: 0.09\n",
      "901 yield label raw: 0.13\n",
      "predicted yield: 0.12, yield label: 0.13\n",
      "902 yield label raw: 0.13\n",
      "predicted yield: 0.15, yield label: 0.13\n",
      "903 yield label raw: 0.01\n",
      "predicted yield: -0.03, yield label: 0.01\n",
      "904 yield label raw: 0.11\n",
      "predicted yield: 0.12, yield label: 0.11\n",
      "905 yield label raw: 0.11\n",
      "predicted yield: 0.11, yield label: 0.11\n",
      "906 yield label raw: 0.04\n",
      "predicted yield: 0.03, yield label: 0.04\n",
      "915 yield label raw: 0.03\n",
      "predicted yield: 0.07, yield label: 0.03\n",
      "927 yield label raw: 0.05\n",
      "predicted yield: 0.12, yield label: 0.05\n",
      "933 yield label raw: 0.02\n",
      "predicted yield: -0.06, yield label: 0.02\n",
      "936 yield label raw: 0.08\n",
      "predicted yield: 0.06, yield label: 0.08\n",
      "945 yield label raw: 0.07\n",
      "predicted yield: 0.07, yield label: 0.07\n",
      "948 yield label raw: 0.01\n",
      "predicted yield: -0.01, yield label: 0.01\n",
      "951 yield label raw: 0.05\n",
      "predicted yield: 0.07, yield label: 0.05\n",
      "955 yield label raw: 0.88\n",
      "predicted yield: 0.81, yield label: 0.88\n",
      "963 yield label raw: 0.01\n",
      "predicted yield: 0.07, yield label: 0.01\n",
      "966 yield label raw: 0.02\n",
      "predicted yield: -0.06, yield label: 0.02\n",
      "970 yield label raw: 0.84\n",
      "predicted yield: 0.74, yield label: 0.84\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-72b8c68ab18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"yield label raw: {round(raw_data[sample_num]['yield']/100,2)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"predicted yield: {predicted }, yield label: {actual }\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mgood_bad\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'bad'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "#sample_num=870\n",
    "bad_preds=[]\n",
    "#for sample_num in samples:#len(raw_data))\n",
    "for sample_num in range(,len(raw_data)):\n",
    "\n",
    "\n",
    "    curr_sample_raw = raw_data[sample_num]\n",
    "    smiles= get_smiles_from_raw_json(curr_sample_raw)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    bond_map = get_bond_map(mol)\n",
    "    yield_label = dataset[sample_num]['yield_label']\n",
    "    \n",
    "\n",
    "    ############### Define hook\n",
    "    save_output = SaveOutput()\n",
    "    hook_handles = [];i=0\n",
    "    for layer in model.modules():\n",
    "        handle = layer.register_forward_hook(save_output)\n",
    "        hook_handles.append(handle)\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "    ############### Do one pass of the model\n",
    "    model.eval()\n",
    "    if yield_label>0.8 or yield_label<0.2:\n",
    "        for i, data in enumerate(dataloader):\n",
    "            if i<=sample_num:\n",
    "                if i==sample_num:\n",
    "                    mask_neis = torch.unsqueeze(data['n_bonds'].unsqueeze(-1) > torch.arange(0, 15, dtype=torch.int32).view(1, 1, -1), -1)\n",
    "                    max_n_atoms = data['n_atoms'].max()\n",
    "                    mask_atoms = torch.unsqueeze(data['n_atoms'].unsqueeze(-1) > torch.arange(0, max_n_atoms, dtype=torch.int32).view(1, -1),-1)\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        yield_scores = model.forward(data['atom_feats'], data['bond_feats'],data['atom_graph'], data['bond_graph'], \n",
    "                        data['n_bonds'],data['n_atoms'], data['binary_feats'], mask_neis, mask_atoms,data['sparse_idx'],data['domain_feats'])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        predicted = round(yield_scores.item(),2)\n",
    "        actual = round(yield_label.item(),2)\n",
    "        gnn_weights= save_output.outputs[11].detach().to('cpu').squeeze(0).squeeze(2).numpy()\n",
    "        ############### plot the figures\n",
    "        if actual>0.8:\n",
    "\n",
    "            if abs(predicted-actual)>0.25:\n",
    "                print(sample_num,f\"yield label raw: {round(raw_data[sample_num]['yield']/100,2)}\")\n",
    "                print(f\"predicted yield: {predicted }, yield label: {actual }\")\n",
    "                good_bad= 'bad'\n",
    "                plot_activations(gnn_weights,sample_num,actual,predicted,good_bad)\n",
    "                bad_preds.append(sample_num)\n",
    "                \n",
    "            #if False:\n",
    "            elif abs(predicted-actual)<0.1:\n",
    "                print(sample_num,f\"yield label raw: {round(raw_data[sample_num]['yield']/100,2)}\")\n",
    "                print(f\"predicted yield: {predicted }, yield label: {actual }\")\n",
    "                good_bad= 'good'\n",
    "                plot_activations(gnn_weights,sample_num,actual,predicted,good_bad)\n",
    "\n",
    "        elif actual <0.5:\n",
    "            \n",
    "            if abs(predicted-actual)>0.25:\n",
    "                print(sample_num,f\"yield label raw: {round(raw_data[sample_num]['yield']/100,2)}\")\n",
    "                print(f\"predicted yield: {predicted }, yield label: {actual }\")\n",
    "                good_bad= 'bad'\n",
    "                plot_activations(gnn_weights,sample_num,actual,predicted,good_bad)\n",
    "                bad_preds.append(sample_num)\n",
    "                \n",
    "            #if False:\n",
    "            elif abs(predicted-actual)<0.1:\n",
    "                print(sample_num,f\"yield label raw: {round(raw_data[sample_num]['yield']/100,2)}\")\n",
    "                print(f\"predicted yield: {predicted }, yield label: {actual }\")\n",
    "                good_bad= 'good'\n",
    "                plot_activations(gnn_weights,sample_num,actual,predicted,good_bad)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#VALUE!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[sample_num]['yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(yield_scores.item(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.04\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"600px\" version=\"1.1\" width=\"600px\" xml:space=\"preserve\" xmlns:rdkit=\"http://www.rdkit.org/xml\" xmlns:svg=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<rect height=\"600\" style=\"opacity:1.0;fill:#FFFFFF;stroke:none\" width=\"600\" x=\"0\" y=\"0\"> </rect>\n",
       "<path d=\"M 200.879,471.403 162.001,492.316\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 162.001,492.316 141.088,453.438\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 162.001,492.316 123.123,513.23\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 227.041,532.522 247.954,571.4\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 247.954,571.4 292.08,572.727\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 292.08,572.727 315.293,535.177\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 315.293,535.177 294.379,496.299\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 317.592,458.748 294.379,496.299\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 313.209,311.034 276.784,286.093\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 232.714,141.967 210.641,103.736\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.641,103.736 232.714,65.5043\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:16px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"200.879\" cy=\"471.403\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"162.001\" cy=\"492.316\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"141.088\" cy=\"453.438\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"227.041\" cy=\"532.522\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"247.954\" cy=\"571.4\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"292.08\" cy=\"572.727\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"315.293\" cy=\"535.177\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"317.592\" cy=\"458.748\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"294.379\" cy=\"496.299\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"123.123\" cy=\"513.23\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"313.209\" cy=\"311.034\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"276.784\" cy=\"286.093\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"232.714\" cy=\"141.967\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"210.641\" cy=\"103.736\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"232.714\" cy=\"65.5043\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<ellipse cx=\"515.069\" cy=\"509.976\" rx=\"11.7723\" ry=\"11.7723\" style=\"fill:#FF7F7F;fill-rule:evenodd;stroke:#FF7F7F;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 191.916,471.212 178.859,478.236\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 178.859,478.236 165.801,485.259\" style=\"fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 196.099,478.987 183.041,486.011\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 183.041,486.011 169.984,493.035\" style=\"fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 161.931,482.867 155.432,470.786\" style=\"fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 155.432,470.786 148.934,458.705\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 154.156,487.05 147.657,474.969\" style=\"fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 147.657,474.969 141.158,462.887\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 165.959,499.674 172.458,511.755\" style=\"fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 172.458,511.755 178.957,523.837\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 156.11,495.486 139.617,504.358\" style=\"fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 139.617,504.358 123.123,513.23\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 189.787,531.401 208.414,531.961\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 208.414,531.961 227.041,532.522\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 227.041,532.522 247.954,571.4\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 237.953,534.171 252.593,561.385\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 227.041,532.522 250.253,494.971\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 247.954,571.4 266.829,571.968\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 266.829,571.968 285.703,572.535\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 296.628,565.37 305.961,550.273\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 305.961,550.273 315.293,535.177\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 291.918,556.198 298.45,545.631\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 298.45,545.631 304.983,535.063\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 315.293,535.177 359.419,536.504\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 315.293,535.177 294.379,496.299\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 359.419,536.504 382.631,498.954\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 355.391,526.229 371.639,499.944\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 382.631,498.954 361.718,460.076\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 361.718,460.076 317.592,458.748\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 354.834,468.702 323.945,467.773\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 317.592,458.748 294.379,496.299\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 294.379,496.299 250.253,494.971\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 287.495,504.925 256.607,503.996\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 123.123,513.23 114.646,497.47\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 114.646,497.47 106.168,481.71\" style=\"fill:none;fill-rule:evenodd;stroke:#33CCCC;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 123.123,513.23 131.601,528.99\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 131.601,528.99 140.079,544.75\" style=\"fill:none;fill-rule:evenodd;stroke:#33CCCC;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 123.123,513.23 106.383,522.235\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 106.383,522.235 89.6422,531.24\" style=\"fill:none;fill-rule:evenodd;stroke:#33CCCC;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 353.022,291.96 313.209,311.034\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 313.209,311.034 276.784,286.093\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 302.757,314.578 277.26,297.119\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 313.209,311.034 309.821,355.05\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 276.784,286.093 236.972,305.166\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 236.972,305.166 233.584,349.182\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 245.267,312.446 242.895,343.257\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 233.584,349.182 270.009,374.124\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 233.584,349.182 219.284,360.194\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 219.284,360.194 204.984,371.206\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 270.009,374.124 257.543,416.474\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 270.009,374.124 309.821,355.05\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 272.166,363.3 300.035,349.949\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 257.543,416.474 238.668,417\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 238.668,417 219.792,417.527\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 251.634,407.806 238.421,408.175\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 238.421,408.175 225.208,408.544\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.795,410.348 201.226,383.475\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 192.229,374.24 174.243,368.946\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 174.243,368.946 156.257,363.652\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 156.257,363.652 145.878,320.743\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 156.257,363.652 143.708,375.601\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 143.708,375.601 131.159,387.551\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 145.878,320.743 103.528,308.278\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 103.528,308.278 71.558,338.721\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 71.558,338.721 81.9376,381.63\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 81.9376,381.63 99.6764,386.851\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 99.6764,386.851 117.415,392.072\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 309.821,355.05 340.355,375.958\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 345.68,387.35 344.552,402\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 344.552,402 343.425,416.65\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 352.138,377.17 362.474,372.218\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 362.474,372.218 372.81,367.266\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 365.152,141.967 343.079,180.199\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 354.195,143.287 338.744,170.05\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 365.152,141.967 343.079,103.736\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 343.079,180.199 298.933,180.199\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 298.933,180.199 276.86,141.967\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 303.269,170.05 287.818,143.287\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 276.86,141.967 257.733,141.967\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 257.733,141.967 238.606,141.967\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 276.86,141.967 298.933,103.736\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 228.466,134.61 219.554,119.173\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 219.554,119.173 210.641,103.736\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 228.466,149.325 219.554,164.762\" style=\"fill:none;fill-rule:evenodd;stroke:#FF7F00;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 219.554,164.762 210.641,180.199\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.641,103.736 232.714,65.5043\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 206.306,93.5865 221.757,66.8244\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.641,103.736 166.495,103.736\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 232.714,65.5043 210.641,27.2727\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.641,27.2727 166.495,27.2727\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 204.02,36.1019 173.117,36.1019\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 166.495,27.2727 144.422,65.5043\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 144.422,65.5043 166.495,103.736\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 155.38,66.8244 170.831,93.5865\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.641,180.199 166.495,180.199\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 204.02,189.028 173.117,189.028\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.641,180.199 232.714,218.43\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 166.495,180.199 144.422,218.43\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 144.422,218.43 166.495,256.662\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 155.38,219.751 170.831,246.513\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 166.495,256.662 210.641,256.662\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 210.641,256.662 232.714,218.43\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 206.306,246.513 221.757,219.751\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 298.933,103.736 343.079,103.736\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 305.555,112.565 336.457,112.565\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 412.062,509.976 427.51,509.976\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<path d=\"M 427.51,509.976 442.959,509.976\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000\" x=\"194.007\" y=\"478.761\"><tspan>O</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#CCCC00\" x=\"156.11\" y=\"499.674\"><tspan>S</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000\" x=\"134.216\" y=\"460.796\"><tspan>O</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000\" x=\"176.043\" y=\"538.552\"><tspan>O</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#0000FF\" x=\"285.703\" y=\"580.085\"><tspan>N</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#33CCCC\" x=\"96.8132\" y=\"481.71\"><tspan>F</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#33CCCC\" x=\"138.64\" y=\"559.465\"><tspan>F</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#33CCCC\" x=\"78.8486\" y=\"541.501\"><tspan>F</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#0000FF\" x=\"207.037\" y=\"425.063\"><tspan>N</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#0000FF\" x=\"192.229\" y=\"383.475\"><tspan>N</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000\" x=\"117.415\" y=\"401.453\"><tspan>O</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#000000\" x=\"340.355\" y=\"387.35\"><tspan>B</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000\" x=\"329.609\" y=\"431.365\"><tspan>OH</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000\" x=\"372.81\" y=\"368.276\"><tspan>OH</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF7F00\" x=\"226.823\" y=\"149.325\"><tspan>P</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000\" x=\"442.959\" y=\"517.333\"><tspan>OH</tspan></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#33CCCC\" x=\"478.771\" y=\"518.069\"><tspan>F</tspan><tspan style=\"baseline-shift:super;font-size:10.5px;\">-</tspan><tspan/></text>\n",
       "<text style=\"font-size:14px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#000000\" x=\"501.697\" y=\"518.069\"><tspan>Cs</tspan><tspan style=\"baseline-shift:super;font-size:10.5px;\">+</tspan><tspan/></text>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "res=[]\n",
    "\n",
    "for i in range(len(save_output.outputs)):\n",
    "    np.save('data/image_'+str(i)+'.npy', save_output.outputs[i].detach().to('cpu').numpy())\n",
    "    #res.append(save_output.outputs[i].detach().to('cpu').numpy())\n",
    "#image= save_output.outputs.detach().to('cpu').numpy()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def iterate(model,optimizer,epoch, data_loader, train=True,valid=False):\n",
    "    avg_loss, tmp_r2 =0.0, 0\n",
    "    iters = len(data_loader)\n",
    "    n_samples = len(data_loader.dataset)\n",
    "    correct_yields , pred_yields =np.array([[0]]), np.array([[0]])\n",
    "    model.eval()\n",
    "    if True:\n",
    "        for i, data in enumerate(data_loader):\n",
    "            mask_neis = torch.unsqueeze(data['n_bonds'].unsqueeze(-1) > torch.arange(0, 15, dtype=torch.int32).view(1, 1, -1), -1)\n",
    "            max_n_atoms = data['n_atoms'].max()\n",
    "            mask_atoms = torch.unsqueeze(data['n_atoms'].unsqueeze(-1) > torch.arange(0, max_n_atoms, dtype=torch.int32).view(1, -1),-1)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                yield_scores = model.forward(data['atom_feats'], data['bond_feats'],data['atom_graph'], \n",
    "                                                    data['bond_graph'], data['n_bonds'],data['n_atoms'], data['binary_feats'], \n",
    "                                                    mask_neis, mask_atoms,data['sparse_idx'],data['domain_feats'])\n",
    "                criteria=nn.MSELoss()\n",
    "                loss= criteria(yield_scores, data['yield_label'])\n",
    "                loss = torch.mean(loss)\n",
    "                avg_loss += loss.item()\n",
    "                aa=data['yield_label'].cpu().detach().numpy()\n",
    "                bb=yield_scores.cpu().detach().numpy()\n",
    "                correct_yields=np.append(correct_yields,aa)\n",
    "                pred_yields=np.append(pred_yields,bb)\n",
    "\n",
    "        tmp_r2=r2_score(correct_yields,pred_yields)\n",
    "        logging.info(\"Epoch: {:2d}  Loss: {:f}  R2: {:6.2%} \".format(epoch,avg_loss, tmp_r2))\n",
    "        return correct_yields,pred_yields,tmp_r2,avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/21 11:35:08: Epoch:  1  Loss: 28.884723  R2: 91.86% \n"
     ]
    }
   ],
   "source": [
    "correct_yields,pred_yields, r2 ,loss= iterate( model,optimizer,1, dy_dataloader, train=False,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/21 11:40:26: Epoch:  1  Loss: 133.645681  R2: -103.15% \n"
     ]
    }
   ],
   "source": [
    "correct_yields,pred_yields, r2 ,loss= iterate( model,optimizer,1, az_dataloader, train=False,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/02/21 11:42:58: Epoch:  1  Loss: 872.601692  R2: -128.78% \n"
     ]
    }
   ],
   "source": [
    "correct_yields,pred_yields, r2 ,loss= iterate( model,optimizer,1, su_dataloader, train=False,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11045819235104222, 0.07012773506296444)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_yields),np.std(pred_yields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4178988746543945, 0.2872947228123882)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(correct_yields),np.std(correct_yields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59, 0.59]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([round(i,2) for i in list(pred_yields)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/'+args.output_name+'_y_true.txt', 'w') as f_true, open('data/'+args.output_name+'_y_pred.txt', 'w') as f_pred:\n",
    "    f_true.write(','.join(map(str, [float(n[0]) for n in a2])))\n",
    "    f_pred.write(','.join(map(str, [float(n[0]) for n in b2])))\n",
    "    \n",
    "    \n",
    "with open('data/'+args.output_name+'_train_scores.txt', 'w') as train_r2, open('data/'+args.output_name+'_test_scores.txt', 'w') as test_r2:\n",
    "    train_r2.write(','.join(map(str, [float(n) for n in train_scores])))\n",
    "    test_r2.write(','.join(map(str, [float(n) for n in test_scores])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score( data['yield_label'].cpu(), yield_scores.cpu().detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x300 at 0x7FB2F30072E8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit.Chaem import Draw\n",
    "Draw.MolToImage(mol)\n",
    "#Chem.FindMolChiralCenters(mol,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 0\n",
      "C 1\n",
      "C 2\n",
      "C 3\n",
      "C 4\n",
      "C 5\n",
      "C 6\n",
      "C 7\n",
      "Br 8\n"
     ]
    }
   ],
   "source": [
    "m = Chem.MolFromMolFile('data/doyle_reaction_mols/1-bromo-4-ethylbenzene.mol')\n",
    "m.GetNumAtoms()\n",
    "AllChem.Compute2DCoords(m)\n",
    "for atom in m.GetAtoms():\n",
    "    print(atom.GetSymbol(),atom.GetIdx())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_atom_mapping_doyle(mol_dir):\n",
    "    \n",
    "    smiles_mapping=defaultdict(dict) \n",
    "    \n",
    "    for mol_fn in glob.glob(mol_dir):\n",
    "        atoms , labels, atom_mapping =[], [], []\n",
    "        if 'methyl-isoxazole-5-carboxylate' in mol_fn:\n",
    "            print(mol_fn)\n",
    "            m = Chem.MolFromMolFile(mol_fn)\n",
    "            smiles=Chem.MolToSmiles(m)\n",
    "            mol_lines=open(mol_fn,'r').readlines()\n",
    "\n",
    "            for i,line in enumerate(mol_lines[4:]):\n",
    "                l=re.sub(' +', ' ', line.strip('\\n'))\n",
    "                l2=l.split(' ')\n",
    "                if len(l2)==17:\n",
    "                    atom=l2[4]\n",
    "                    if atom !='' and '*' not in atom and 'H' not in atom:\n",
    "                        atoms.append(atom)\n",
    "                if \"atom_labels\" in line:\n",
    "                    labels=[i.strip('*') for i in mol_lines[i+1+4].strip('\\n').split(' ') if ((i!='') and ('H' not in i))]\n",
    "                    print(labels)\n",
    "            if len(atoms)==len(labels):\n",
    "                for i in range(len(atoms)):\n",
    "                    atom_mapping.append((atoms[i],labels[i]))            \n",
    "                print(atom_mapping)\n",
    "            else:\n",
    "                print('Atoms and lables don\\'t match')\n",
    "                atom_mapping=[]\n",
    "\n",
    "            if smiles not in smiles_mapping:\n",
    "                print(mol_fn.split('/')[-1].split('.')[0])\n",
    "                smiles_mapping[mol_fn.split('/')[-1].split('.')[0]]=atom_mapping\n",
    "            else:\n",
    "                print(\"key exsists\")\n",
    "    return smiles_mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te\n"
     ]
    }
   ],
   "source": [
    "for mol_fn in glob.glob(\"data/doyle_reaction_mols/*.mol\"):\n",
    "    if 'methyl-isoxazole-5-carboxylate' in mol_fn:\n",
    "        print(\"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/doyle_reaction_mols/methyl-isoxazole-5-carboxylate.mol\n",
      "['O1', 'N1', 'C3', 'C4', 'C5', 'C1', 'O1', 'O2', 'C2']\n",
      "[('O', 'O1'), ('N', 'N1'), ('C', 'C3'), ('C', 'C4'), ('C', 'C5'), ('C', 'C1'), ('O', 'O1'), ('O', 'O2'), ('C', 'C2')]\n",
      "methyl-isoxazole-5-carboxylate\n"
     ]
    }
   ],
   "source": [
    "smiles_mapping=get_atom_mapping_doyle(\"data/doyle_reaction_mols/*.mol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 'O1'),\n",
       " ('N', 'N1'),\n",
       " ('C', 'C3'),\n",
       " ('C', 'C4'),\n",
       " ('C', 'C5'),\n",
       " ('C', 'C1'),\n",
       " ('O', 'O1'),\n",
       " ('O', 'O2'),\n",
       " ('C', 'C2')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_mapping['methyl-isoxazole-5-carboxylate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_mapping['methyl-isoxazole-5-carboxylate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 'C1'),\n",
       " ('C', 'C4'),\n",
       " ('C', 'C2'),\n",
       " ('C', 'C2'),\n",
       " ('C', 'C3'),\n",
       " ('C', 'C3'),\n",
       " ('C', 'C4'),\n",
       " ('C', 'C1'),\n",
       " ('Br', 'Br1')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_mapping['1-bromo-4-ethylbenzene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms=[{\"name\": \"C1\", \"atomic_num\": 6, \"partial_charge\": -0.025, \"nmr_shift\": 132.85}, {\"name\": \"C2\", \"atomic_num\": 6, \"partial_charge\": -0.058, \"nmr_shift\": 125.36}, {\"name\": \"C3\", \"atomic_num\": 6, \"partial_charge\": -0.211, \"nmr_shift\": 122.91}, {\"name\": \"C4\", \"atomic_num\": 6, \"partial_charge\": 0.167, \"nmr_shift\": 136.02}, {\"name\": \"H2\", \"atomic_num\": 1, \"partial_charge\": 0.104, \"nmr_shift\": 7.11}, {\"name\": \"H3\", \"atomic_num\": 1, \"partial_charge\": 0.14, \"nmr_shift\": 6.97}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_charge_dict=defaultdict(float)\n",
    "nmr_dict=defaultdict(float)\n",
    "for atom in atoms:\n",
    "    p_charge_dict[atom['name']]=atom['partial_charge']\n",
    "    nmr_dict[atom['name']]=atom['nmr_shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'C1': -0.025,\n",
       "             'C2': -0.058,\n",
       "             'C3': -0.211,\n",
       "             'C4': 0.167,\n",
       "             'H2': 0.104,\n",
       "             'H3': 0.14})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_charge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Chem.MolFromMolFile('data/doyle_reaction_mols/1-bromo-4-ethylbenzene.mol')\n",
    "partial_charges=[]\n",
    "nmr_shifts=[]\n",
    "\n",
    "for atom in m.GetAtoms():\n",
    "    atom_label=smiles_mapping['1-bromo-4-ethylbenzene'][atom.GetIdx()][1]\n",
    "    partial_charges.append(p_charge_dict[atom_label])\n",
    "    nmr_shifts.append(nmr_dict[atom_label])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.025, 0.167, -0.058, -0.058, -0.211, -0.211, 0.167, -0.025, 0.0]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[132.85, 136.02, 125.36, 125.36, 122.91, 122.91, 136.02, 132.85, 0.0]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmr_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=Chem.MolFromSmiles('CCc1ccc(Br)cc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem._ROAtomSeq at 0x7fd7fa850d50>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.GetAtoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 C\n",
      "1 C\n",
      "2 C\n",
      "3 C\n",
      "4 C\n",
      "5 C\n",
      "6 Br\n",
      "7 C\n",
      "8 C\n"
     ]
    }
   ],
   "source": [
    "for atom in mm.GetAtoms():\n",
    "    print(atom.GetIdx(),atom.GetSymbol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chem.MolFromMolFile('data/doyle_reaction_mols/1-bromo-4-ethylbenzene.mol')\n",
    "m.GetNumAtoms()\n",
    "Chem.MolToSmiles(m)\n",
    "#AllChem.Compute2DCoords(m2)\n",
    "#for atom in m.GetAtoms():\n",
    "    #print(atom.GetSymbol(),atom.GetIdx())\n",
    "#print(Chem.MolToMolBlock(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "file_name = args.train_dataset\n",
    "path = args.dataset_path\n",
    "rxns = []\n",
    "degree_codec = LabelEncoder()\n",
    "symbol_codec = LabelEncoder()\n",
    "expl_val_codec = LabelEncoder()\n",
    "bond_type_codec = LabelEncoder()\n",
    "max_nbonds = 10   \n",
    "symbols = set()\n",
    "degrees = set()\n",
    "explicit_valences = set()\n",
    "bond_types = set()\n",
    "\n",
    "a=True\n",
    "charges,shifts=[math.inf,-math.inf],[math.inf,-math.inf]\n",
    "with open(os.path.join(path, file_name)) as datafile:\n",
    "    data = json.load(datafile)\n",
    "    for line in data:\n",
    "\n",
    "        product=line['product']\n",
    "        reactants=line['reactants']\n",
    "        r_yield=line['yield']\n",
    "     \n",
    "        rxn = Rxn(product,reactants,r_yield)\n",
    "        mol = Chem.MolFromSmiles(rxn.reactants_smile)\n",
    "        #atom_idx = torch.tensor([atom.GetIdx()-1 for atom in mol.GetAtoms()], dtype=torch.int64)\n",
    "\n",
    "        mol_reactants=rxn.reactants\n",
    "        for mol_idx in range(1,len(mol_reactants)):\n",
    "            current_molecule=mol_reactants[mol_idx]\n",
    "            atoms=current_molecule.atoms\n",
    "            #p_charge_dict=defaultdict(float)\n",
    "            #nmr_dict=defaultdict(float)\n",
    "            for atom in atoms:\n",
    "                if 'partial_charge' in atom:\n",
    "                    charges[0],charges[1]= min(charges[0],atom['partial_charge']), max(charges[1],atom['partial_charge'])\n",
    "                if 'nmr_shift' in atom:\n",
    "                    shifts[0],shifts[1]= min(shifts[0],atom['nmr_shift']), max(shifts[1],atom['nmr_shift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.96, 1.858]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.41, 168.89]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yieldrxn",
   "language": "python",
   "name": "yieldrxn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
